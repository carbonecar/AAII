{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "914b40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear una red neuronal de una capa (perceptron) para predecir sore el dataset wine \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.api import add_constant as ac\n",
    "from statsmodels.api import OLS, GLM\n",
    "from statsmodels.formula.api import ols, glm\n",
    "from statsmodels.tools import add_constant\n",
    "from statsmodels.datasets import get_rdataset as load_data\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd   \n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.diagnostic import lilliefors\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "932c44e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "0                            3.92   1065.0       0  \n",
       "1                            3.40   1050.0       0  \n",
       "2                            3.17   1185.0       0  \n",
       "3                            3.45   1480.0       0  \n",
       "4                            2.93    735.0       0  \n",
       "..                            ...      ...     ...  \n",
       "173                          1.74    740.0       2  \n",
       "174                          1.56    750.0       2  \n",
       "175                          1.56    835.0       2  \n",
       "176                          1.62    840.0       2  \n",
       "177                          1.60    560.0       2  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>12.07</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.17</td>\n",
       "      <td>21.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.28</td>\n",
       "      <td>378.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>12.43</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.29</td>\n",
       "      <td>21.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.69</td>\n",
       "      <td>2.84</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>11.79</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.78</td>\n",
       "      <td>28.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.44</td>\n",
       "      <td>466.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>12.37</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.30</td>\n",
       "      <td>24.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.78</td>\n",
       "      <td>342.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>12.04</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.38</td>\n",
       "      <td>22.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.57</td>\n",
       "      <td>580.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "125    12.07        2.16  2.17               21.0       85.0           2.60   \n",
       "126    12.43        1.53  2.29               21.5       86.0           2.74   \n",
       "127    11.79        2.13  2.78               28.5       92.0           2.13   \n",
       "128    12.37        1.63  2.30               24.5       88.0           2.22   \n",
       "129    12.04        4.30  2.38               22.0       80.0           2.10   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "125        2.65                  0.37             1.35             2.76  0.86   \n",
       "126        3.15                  0.39             1.77             3.94  0.69   \n",
       "127        2.24                  0.58             1.76             3.00  0.97   \n",
       "128        2.45                  0.40             1.90             2.12  0.89   \n",
       "129        1.75                  0.42             1.35             2.60  0.79   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "0                            3.92   1065.0       0  \n",
       "1                            3.40   1050.0       0  \n",
       "2                            3.17   1185.0       0  \n",
       "3                            3.45   1480.0       0  \n",
       "4                            2.93    735.0       0  \n",
       "..                            ...      ...     ...  \n",
       "125                          3.28    378.0       1  \n",
       "126                          2.84    352.0       1  \n",
       "127                          2.44    466.0       1  \n",
       "128                          2.78    342.0       1  \n",
       "129                          2.57    580.0       1  \n",
       "\n",
       "[130 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wine_raw=load_wine()\n",
    "\n",
    "wine_df=pd.DataFrame(data=wine_raw.data, columns=wine_raw.feature_names)\n",
    "wine_df\n",
    "\n",
    "wine_df['target'] = wine_raw.target\n",
    "display(wine_df)\n",
    "#Eliminamos las clases que son 2 asi queda binario\n",
    "wine_df=wine_df[wine_df['target']!=2]\n",
    "display(wine_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fbf48015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.030303030303030304\n",
      "Test Accuracy (model.score): 0.969697\n",
      "Train Accuracy (model.score): 1.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "X=wine_raw.data\n",
    "y=wine_raw.target\n",
    "X=X[y!=2]\n",
    "y=y[y!=2]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train_p1=np.where(y_train==1,1,-1)\n",
    "y_test_p1=np.where(y_test==1,1,-1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Mean Squared Error: {mse(y_test, y_pred)}')\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "print(f'Test Accuracy (model.score): {test_accuracy:0.6f}')\n",
    "print(f'Train Accuracy (model.score): {train_accuracy:0.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1a20d485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.3731\n",
      "Epoch [200/1000], Loss: 0.2428\n",
      "Epoch [300/1000], Loss: 0.1890\n",
      "Epoch [400/1000], Loss: 0.1582\n",
      "Epoch [500/1000], Loss: 0.1376\n",
      "Epoch [600/1000], Loss: 0.1226\n",
      "Epoch [700/1000], Loss: 0.1111\n",
      "Epoch [800/1000], Loss: 0.1018\n",
      "Epoch [900/1000], Loss: 0.0942\n",
      "Epoch [1000/1000], Loss: 0.0878\n",
      "Test Accuracy: 0.9394\n",
      "Train Accuracy: 0.9897\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1) ## Se le agrega una dimension porque lo requiere asi pytorch\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "class Percepton(nn.Module):\n",
    "    def __init__(self, input_dim, p_drop=0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p_drop) ## Dropout no es necesario en una sola capa pero se deja para referencia\n",
    "        self.layer = nn.Linear(input_dim, 1)\n",
    "        self.activation = nn.Sigmoid()  ## usamos la funcion sigmoid porque queremos claificar la salida como 0 Ã³ 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x=self.dropout(x)\n",
    "        x = self.layer(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "input_dim =X_train_tensor.shape[1]\n",
    "model = Percepton(input_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss=criterion(outputs, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor)\n",
    "    y_pred_test = model(X_test_tensor)\n",
    "    y_pred_train_class = (y_pred_train.numpy() > 0.5).astype(int)\n",
    "    y_pred_test_class = (y_pred_test.numpy() > 0.5).astype(int)\n",
    "    train_accuracy = (y_pred_train_class == y_train_tensor.numpy()).mean()\n",
    "    test_accuracy = (y_pred_test_class == y_test_tensor.numpy()).mean()\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    print(f'Train Accuracy: {train_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a2e0c",
   "metadata": {},
   "source": [
    "- Implementar la clase MLP3 percepton multicapa de 3 capas densas\n",
    "- Entrenar y evaluar y compara los resultados para MLP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ddd6caed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.6324\n",
      "Epoch [200/1000], Loss: 0.5386\n",
      "Epoch [300/1000], Loss: 0.4082\n",
      "Epoch [400/1000], Loss: 0.2698\n",
      "Epoch [500/1000], Loss: 0.2254\n",
      "Epoch [600/1000], Loss: 0.1451\n",
      "Epoch [700/1000], Loss: 0.1367\n",
      "Epoch [800/1000], Loss: 0.1150\n",
      "Epoch [900/1000], Loss: 0.0707\n",
      "Epoch [1000/1000], Loss: 0.0467\n",
      "Test Accuracy: 0.9697\n",
      "Train Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "## Creamos un clase MLP3 percepton multicapa de 3 capas densas\n",
    "class MLP3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=16, hidden_dim2=8, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.layer2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.layer3 = nn.Linear(hidden_dim2, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "input_dim =X_train_tensor.shape[1]\n",
    "mlp3 = MLP3(input_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(mlp3.parameters(), lr=0.01)\n",
    "\n",
    "mlp3.train()\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    outputs = mlp3(X_train_tensor)\n",
    "    loss=criterion(outputs, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "mlp3.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train = mlp3(X_train_tensor)\n",
    "    y_pred_test = mlp3(X_test_tensor)\n",
    "    y_pred_train_class = (y_pred_train.numpy() > 0.5).astype(int)\n",
    "    y_pred_test_class = (y_pred_test.numpy() > 0.5).astype(int)\n",
    "    train_accuracy = (y_pred_train_class == y_train_tensor.numpy()).mean()\n",
    "    test_accuracy = (y_pred_test_class == y_test_tensor.numpy()).mean()\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    print(f'Train Accuracy: {train_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45275083",
   "metadata": {},
   "source": [
    "Ahora voy a exportar esta red para poder dibujarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0eecf759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagrama guardado en mlp3_graph.png\n"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "dummy = torch.randn(1, input_dim)\n",
    "logits = mlp3(dummy)\n",
    "dot = make_dot(logits, params=dict(mlp3.named_parameters()))\n",
    "dot.render(\"mlp3_graph\", format=\"png\")  # genera mlp3_graph.png\n",
    "print(\"Diagrama guardado en mlp3_graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5d8c9888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportado a mlp3.onnx (abrilo en netron.app)\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.randn(1, input_dim)\n",
    "torch.onnx.export(\n",
    "    mlp3, dummy, \"mlp3.onnx\",\n",
    "    input_names=[\"input\"], output_names=[\"logits\"],\n",
    "    opset_version=17, do_constant_folding=True\n",
    ")\n",
    "print(\"Exportado a mlp3.onnx (abrilo en netron.app)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd534ea",
   "metadata": {},
   "source": [
    "Hacemos otra version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "9e9c3141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000]  Loss: 0.6784\n",
      "Epoch [200/1000]  Loss: 0.6498\n",
      "Epoch [300/1000]  Loss: 0.5992\n",
      "Epoch [400/1000]  Loss: 0.5383\n",
      "Epoch [500/1000]  Loss: 0.4786\n",
      "Epoch [600/1000]  Loss: 0.4185\n",
      "Epoch [700/1000]  Loss: 0.3519\n",
      "Epoch [800/1000]  Loss: 0.2687\n",
      "Epoch [900/1000]  Loss: 0.3126\n",
      "Epoch [1000/1000]  Loss: 0.2647\n",
      "Test Accuracy:  0.9394\n",
      "Train Accuracy: 1.0000\n",
      "Diagrama guardado en mlp3_graph_2.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "# --- Modelo ---\n",
    "class MLP3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=16, hidden_dim2=8, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.layer2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.layer3 = nn.Linear(hidden_dim2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)              # <-- sin Sigmoid aquÃ­\n",
    "        return x                        # logits\n",
    "\n",
    "# --- Datos (asegurate de dtypes/shapes) ---\n",
    "# X_train_tensor: torch.float32, shape (n_samples, input_dim)\n",
    "# y_train_tensor: torch.float32, shape (n_samples, 1)\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "y_train_tensor = y_train_tensor.to(torch.float32).view(-1, 1)\n",
    "y_test_tensor  = y_test_tensor.to(torch.float32).view(-1, 1)\n",
    "\n",
    "mlp3 = MLP3(input_dim)\n",
    "\n",
    "# PÃ©rdida estable para binario con logits\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(mlp3.parameters(), lr=0.01)\n",
    "\n",
    "# --- Entrenamiento ---\n",
    "mlp3.train()\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = mlp3(X_train_tensor)\n",
    "    loss = criterion(logits, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}]  Loss: {loss.item():.4f}')\n",
    "\n",
    "# --- EvaluaciÃ³n ---\n",
    "mlp3.eval()\n",
    "with torch.no_grad():\n",
    "    # Probabilidades = sigmoid(logits)\n",
    "    y_pred_train_prob = torch.sigmoid(mlp3(X_train_tensor))\n",
    "    y_pred_test_prob  = torch.sigmoid(mlp3(X_test_tensor))\n",
    "\n",
    "    y_pred_train_class = (y_pred_train_prob > 0.5).int()\n",
    "    y_pred_test_class  = (y_pred_test_prob  > 0.5).int()\n",
    "\n",
    "    train_accuracy = (y_pred_train_class.cpu().numpy() == y_train_tensor.cpu().numpy()).mean()\n",
    "    test_accuracy  = (y_pred_test_class.cpu().numpy()  == y_test_tensor.cpu().numpy()).mean()\n",
    "\n",
    "    print(f'Test Accuracy:  {test_accuracy:.4f}')\n",
    "    print(f'Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "# --- Graficar con torchviz ---\n",
    "# Usamos un batch dummy (puede ser un batch real tambiÃ©n)\n",
    "dummy = torch.randn(1, input_dim)\n",
    "logits = mlp3(dummy)\n",
    "dot = make_dot(logits, params=dict(mlp3.named_parameters()))\n",
    "dot.render(\"mlp3_graph\", format=\"png\")  # genera mlp3_graph.png\n",
    "print(\"Diagrama guardado en mlp3_graph_2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## crear un percepton con pytorch\n",
    "\n",
    "class Perceptron2(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
